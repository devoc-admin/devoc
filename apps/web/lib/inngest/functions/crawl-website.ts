import { eq } from "drizzle-orm";
import { WebCrawler } from "@/lib/crawler";
import { db } from "@/lib/db";
import { crawledPage, crawlJob } from "@/lib/db/schema";
import { inngest } from "../client";

type CrawlRequestEvent = {
  data: {
    crawlJobId: string;
    url: string;
    config: {
      maxDepth: number;
      maxPages: number;
      delayBetweenRequests: number;
      respectRobotsTxt: boolean;
      includePaths?: string[];
      excludePaths?: string[];
    };
  };
};

export const crawlWebsite = inngest.createFunction(
  {
    id: "crawl-website",
    name: "Crawl Website for audit",
    retries: 2,
  },
  { event: "audit/crawl.requested" },
  async ({ event, step, logger }) => {
    const { crawlJobId, url, config } = event.data as CrawlRequestEvent["data"];

    logger.info("ğŸš€ Starting crawl job", {
      crawlJobId,
      maxDepth: config.maxDepth,
      maxPages: config.maxPages,
      url,
    });

    // 1ï¸âƒ£ ğŸƒ Mark as running
    await step.run("mark-job-running", async () => {
      const nowString = new Date().toISOString();
      logger.info("ğŸ“ Marking job as running", { crawlJobId });
      await db
        .update(crawlJob)
        .set({
          startedAt: nowString,
          status: "running",
          updatedAt: nowString,
        })
        .where(eq(crawlJob.id, crawlJobId));
    });

    // 2ï¸âƒ£ ğŸƒ Running
    logger.info(`ğŸ•·ï¸ Starting crawler execution for website : ${url}`);
    const result = await step.run("execute-crawl", async () => {
      const crawler = new WebCrawler({
        baseUrl: url,
        config,
        crawlJobId,
      });

      // â³ Update progress
      return await crawler.crawl({
        onProgress: async (progress) => {
          logger.info(
            `ğŸ”¬ ${progress.currentTitle ? `${progress.currentTitle} (${progress.currentUrl})` : progress.currentUrl}  has been parsed`
          );
          logger.info(
            `ğŸ“Š ${progress.crawled}/${progress.discovered} pages crawled (${Math.round(
              (progress.crawled / config.maxPages) * 100
            )}%)`
          );

          const nowString = new Date().toISOString();
          await db
            .update(crawlJob)
            .set({
              pagesCrawled: progress.crawled,
              pagesDiscovered: progress.discovered,
              updatedAt: nowString,
            })
            .where(eq(crawlJob.id, crawlJobId));
        },
      });
    });

    logger.info("âœ… Crawl execution completed", {
      errorsCount: result.errors?.length ?? 0,
      pagesFound: result.pages.length,
    });

    for (const error of result.errors ?? []) {
      logger.error("ğŸš« Error during crawl", {
        error,
      });
    }

    // 3ï¸âƒ£ ğŸ’¾ ğŸ“„ Store crawled pages
    await step.run("save-pages", async () => {
      if (result.pages.length === 0) {
        logger.warn(`âš ï¸ No pages to save (crawlJobId: ${crawlJobId})`);
        return;
      }

      logger.info(
        `ğŸ’¾ Saving crawled pages to database (${result.pages.length})`
      );

      const pagesToInsert = result.pages.map((page) => ({
        category: page.category,
        categoryConfidence: page.categoryConfidence,
        contentType: page.contentType,
        crawlJobId,
        depth: page.depth,
        hasAuthentication: page.characteristics.hasAuthentication,
        hasDocuments: page.characteristics.hasDocuments,
        hasForm: page.characteristics.hasForm,
        hasMultimedia: page.characteristics.hasMultimedia,
        hasTable: page.characteristics.hasTable,
        httpStatus: page.httpStatus,
        layoutSignature: page.characteristics.layoutSignature,
        normalizedUrl: page.normalizedUrl,
        responseTime: page.responseTime,
        screenshotUrl: page.screenshotUrl,
        selectedForAudit: false,
        title: page.title,
        url: page.url,
      }));

      await db.insert(crawledPage).values(pagesToInsert);
    });

    // 4ï¸âƒ£ Select pages for RGAA audit
    await step.run("select-pages-for-audit", async () => {
      logger.info("ğŸ¯ Selecting pages for RGAA audit");

      const mandatoryCategories = [
        "homepage",
        "contact",
        "legal_notices",
        "accessibility",
        "sitemap",
        "help",
        "authentication",
      ] as const;

      // Select first page for each mandatory category
      const selectedMandatory: string[] = [];
      for (const mandatoryCategory of mandatoryCategories) {
        const selectedPage = result.pages.find(
          (page) => page.category === mandatoryCategory
        );
        if (selectedPage) {
          selectedMandatory.push(mandatoryCategory);
          await db
            .update(crawledPage)
            .set({ selectedForAudit: true })
            .where(eq(crawledPage.normalizedUrl, selectedPage.normalizedUrl));
        }
      }

      logger.info("ğŸ“‹ Mandatory categories selected", {
        found: selectedMandatory,
        missing: mandatoryCategories.filter(
          (c) => !selectedMandatory.includes(c)
        ),
      });

      // âœ¨ Select pages with unique characteristics
      const specialPages = result.pages.filter(
        (page) =>
          page.characteristics.hasMultimedia ||
          page.characteristics.hasTable ||
          page.characteristics.hasForm ||
          page.characteristics.hasDocuments
      );

      const maxSpecialPages = 15;
      const selectedSpecial = specialPages.slice(0, maxSpecialPages);

      for (const page of selectedSpecial) {
        await db
          .update(crawledPage)
          .set({ selectedForAudit: true })
          .where(eq(crawledPage.normalizedUrl, page.normalizedUrl));
      }

      logger.info("âœ¨ Special pages selected", {
        characteristics: selectedSpecial.map((p) => ({
          hasDocuments: p.characteristics.hasDocuments,
          hasForm: p.characteristics.hasForm,
          hasMultimedia: p.characteristics.hasMultimedia,
          hasTable: p.characteristics.hasTable,
          url: p.normalizedUrl,
        })),
        selected: selectedSpecial.length,
        totalSpecialFound: specialPages.length,
      });
    });

    //5ï¸âƒ£ ğŸ‰ Mark job as completed
    await step.run("mark-job-completed", async () => {
      logger.info("ğŸ‰ Marking job as completed", { crawlJobId });

      const nowString = new Date().toISOString();
      await db
        .update(crawlJob)
        .set({
          completedAt: nowString,
          pagesCrawled: result.pages.length,
          pagesDiscovered: result.pages.length,
          status: "completed",
          updatedAt: nowString,
        })
        .where(eq(crawlJob.id, crawlJobId));
    });

    logger.info("ğŸ Crawl job finished successfully", {
      crawlJobId,
      errorsCount: result.errors?.length ?? 0,
      pagesDiscovered: result.pages.length,
      url,
    });

    return {
      errorsCount: result.errors?.length ?? 0,
      pagesDiscovered: result.pages.length,
      success: true,
    };
  }
);
